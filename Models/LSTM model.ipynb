{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rdVkqlWrrwXn"
   },
   "source": [
    "**Import the previously generated dataset**\n",
    "\n",
    "change path to where your csv file was saved "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uCT81PlfjP0t"
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "df=pd.read_csv('final_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VYTGT0QWsefl"
   },
   "source": [
    "Remove punctuations, URLs and articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "00XtcyOt74E1"
   },
   "outputs": [],
   "source": [
    " \n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string,re\n",
    "string.punctuation\n",
    "def removepunt(text):\n",
    "  puntfree=\"\".join([i for i in text if i not in string.punctuation])\n",
    "  return puntfree\n",
    "df['text']= df['text'].apply(lambda x: removepunt(x))\n",
    "\n",
    "\n",
    "def remove_URL(sample):\n",
    "    \"\"\"Remove URLs from a sample string\"\"\"\n",
    "    return re.sub(r\"http\\S+\", \"\", sample)\n",
    "\n",
    "df['text']=df.text.apply(lambda x: remove_URL(x))\n",
    "\n",
    "articles=['a','an','the']\n",
    "def remove_art(text):\n",
    "    text=[word.lower() for word in text.split() if word not in articles]\n",
    "    \n",
    "    return \" \".join(text)\n",
    "df['text']=df['text'].apply(lambda x: remove_art(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3tzbNSJlsykg"
   },
   "source": [
    "We need this to insert it in embedding layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y0HU-wZj74HM"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from collections import  Counter\n",
    "\n",
    "def counter_words(text):\n",
    "  count=Counter()\n",
    "  for i in text.values:\n",
    "    for word in i.split():\n",
    "      count[word]+=1\n",
    "  return count\n",
    "\n",
    "text=df.text\n",
    "counter=counter_words(text)\n",
    "len(counter)\n",
    "num_words=len(counter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5EcnZeaRs-aG"
   },
   "source": [
    "Create arrays X: feature and y:labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bZGs7HZ074Kp"
   },
   "outputs": [],
   "source": [
    "X=df['text']\n",
    "\n",
    "y = pd.get_dummies(df['intent']).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xer_EkmetcqB"
   },
   "source": [
    "**Split data into training and testing (validation) data**\n",
    "\n",
    "In this case, we have choosen training data and test data to be 70 percent and 30 percent of the dataset respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "60EcM03N74Ow"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X, y, test_size=0.30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aATPKeWtuI8C"
   },
   "source": [
    "Convert words in the feature to tokens.\n",
    "\n",
    "Convert each command (sentence) of feature to a sequence of tokens.\n",
    "\n",
    "Padding is done to make the length of each command sequence equal as this is a requirement to input data into the neural networks. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yMc091PR74RC"
   },
   "outputs": [],
   "source": [
    "\n",
    "from keras.layers import Input, Dense, LSTM, Embedding, Bidirectional\n",
    "from keras.layers import Dropout, Activation, GlobalMaxPooling1D  \n",
    "from keras import initializers, regularizers, optimizers, constraints, layers\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "\n",
    "#crearte a tokenizer object\n",
    "\n",
    "tokenizer=text.Tokenizer(num_words=num_words)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "\n",
    "train_sequence=tokenizer.texts_to_sequences(X_train)\n",
    "\n",
    "  \n",
    "\n",
    "train_paded= pad_sequences(train_sequence, maxlen=50)\n",
    "\n",
    "test_sequence=tokenizer.texts_to_sequences(X_test)\n",
    "test_paded= pad_sequences(test_sequence, maxlen=50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the LSTM model with appropiate input, hidden and output layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TCEASxtU74YT"
   },
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "embedding_size=128\n",
    "\n",
    "model.add(Embedding(num_words, embedding_size, input_length=50))\n",
    "model.add(LSTM(64, return_sequences=True))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(6, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bjpl0xyY74ah"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "opt = keras.optimizers.Adam(learning_rate=3e-4)\n",
    "model.compile(optimizer=opt,loss = 'categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit training data and test the accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TtPyqTAU74d3",
    "outputId": "0f717fa0-4c37-4925-a834-c43c82acc0d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "14755/14755 [==============================] - 5441s 369ms/step - loss: 0.0675 - accuracy: 0.9776 - val_loss: 0.0111 - val_accuracy: 0.9969\n",
      "Epoch 2/2\n",
      "14755/14755 [==============================] - 5462s 370ms/step - loss: 0.0098 - accuracy: 0.9974 - val_loss: 0.0071 - val_accuracy: 0.9979\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f75f5436fd0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_paded, y_train , epochs=5, batch_size=128, validation_data= (test_paded,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model and tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "IMgp4-qRv7PC",
    "outputId": "40ba5936-6bb2-42e7-d1c6-b99baf0d2a67"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/content/drive/MyDrive/intent_LSTM.h5'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pickle\n",
    "\n",
    "model.save('intent_LSTM.h5')\n",
    "\n",
    "\n",
    "\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model and tokenizer.\n",
    "After saving the model, it can be directly loaded (no need to run the corpus and train the model every time) to make predications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "huNNhSRowHeH"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "loaded_model = tf.keras.models.load_model('intent_LSTM.h5')\n",
    "\n",
    "with open('/content/drive/MyDrive/tokenizer.pickle', 'rb') as handle:\n",
    "    loaded_tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict new text by taking input from the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cbGiszgAwHaq",
    "outputId": "445ada16-7696-472d-d70b-d65f8f98afaf",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "classes=['action', 'query', 'monitor', 'query+action', 'triger+action',\n",
    "       'trigger+query']\n",
    "\n",
    "test= input()\n",
    "seq= loaded_tokenizer.texts_to_sequences([test])\n",
    "padded = pad_sequences(seq, maxlen=50)\n",
    "pred = loaded_model.predict(padded)\n",
    "pred=np.argmax(pred)\n",
    "\n",
    "\n",
    "print(classes[pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V7J8ysp0wHS2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "HiWi.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
